{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF data API - 파일 데이터를 학습.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1nQgmQ34sDcE3WETDHmC_do5bKroehqCK","authorship_tag":"ABX9TyNJ0NZSjqvMzhJ69Z/jmlyx"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"R6RcafUNZwkW","colab_type":"text"},"source":["# TF data API - 파일 데이터를 학습\n","TensorFlow의 Data API 를 이용하여 디스크의 파일을 직접 처리하도록 하겠습니다.\n","\n","먼저 기존과 같은 부분들은 먼저 작성해보겠습니다."]},{"cell_type":"code","metadata":{"id":"Ynbcuex2ctFK","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Wu4p_mSYLq1","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow import feature_column\n","\n","path = '/content/drive/My Drive/dnn_tutorial/'\n","#   set LABEL first\n","output_cols = ['vel']\n","#   set traffic column\n","input_cols_num = ['vel_t05', 'vel_t10', 'vel_t15', 'vel_t20',\n","               'vel_t25', 'vel_t30', 'vel_t35',  'vel_t40']\n","input_cols_cat = ['V_ID']\n","input_cols= input_cols_num + input_cols_cat\n","\n","traffic_data = pd.read_csv(path + 'traffic_data_2link.csv', index_col = 0)\n","from sklearn.model_selection import train_test_split\n","\n","train_data, test_data = train_test_split(traffic_data, test_size = 1024)\n","V_ID_list = train_data['V_ID'].unique().tolist()\n","\n","vel_upper_limit = train_data['vel'].quantile(q=0.98)\n","train_data = train_data[train_data['vel']<=vel_upper_limit]\n","\n","from sklearn.preprocessing import MinMaxScaler\n","scaler= MinMaxScaler()\n","scaler.fit(train_data['vel'].values.reshape(-1,1))\n","\n","def normalize_numeric(dataframe, input_col_list, output_col_list):\n","    return_df = dataframe.copy()\n","    for col in input_col_list:\n","        return_df.loc[:, col] = pd.DataFrame(\n","            scaler.transform(return_df[col].values.reshape(-1, 1)),\n","            columns=[col], index=return_df.index)\n","    for col in output_col_list:\n","        col_backup = 'backup_'+col\n","        return_df[col_backup] = return_df[col].copy()\n","    for col in output_col_list:\n","        return_df.loc[:, col] = pd.DataFrame(\n","            scaler.transform(return_df[col].values.reshape(-1, 1)),\n","            columns=[col], index=return_df.index)\n","    return return_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SJq8KBY3b3tv","colab_type":"code","colab":{}},"source":["feature_columns = []\n","for col in input_cols_num:\n","    feature_columns.append(feature_column.numeric_column(col))\n","V_ID_column = feature_column.categorical_column_with_vocabulary_list('V_ID',\n","                                                                     V_ID_list )\n","feature_columns.append(feature_column.indicator_column(V_ID_column))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"307z93UHaKKr","colab_type":"text"},"source":["train_data 와 test_data 를 normalize 한 뒤에 파일로 저장하겠습니다.\n","\n","(각각 traffic_2links_train_normalized.csv, traffic_2links_test_normalized.csv)\n"]},{"cell_type":"code","metadata":{"id":"-eJFL6fSaJFQ","colab_type":"code","colab":{}},"source":["train_data = normalize_numeric(train_data, input_cols_num, output_cols)\n","test_data = normalize_numeric(test_data, input_cols_num, output_cols)\n","train_path = path+'traffic_2links_train_normalized.csv'\n","test_path = path+'traffic_2links_test_normalized.csv'\n","train_data.to_csv(train_path)\n","test_data.to_csv(test_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7vxCFxV-ad5y","colab_type":"text"},"source":["csv_to_dataset 함수를 이용해 csv 파일을 DNN 모델에 파이프라인으로 넘겨줄 수 있습니다.\n","\n","이때, CSV 파일을 line by line 으로 tensor로 변환하는 _parse_line 함수의 작성이 필요합니다.\n"]},{"cell_type":"code","metadata":{"id":"VazXTbVcaeOo","colab_type":"code","colab":{}},"source":["def _parse_line(line, default_types, input_col_list, output, \n","                input_col_list_name):\n","    #    Decode the line into its fields\n","    fields = tf.io.decode_csv(\n","        line, record_defaults=default_types, select_cols=input_col_list)\n","    #    Pack the result into a dictionary\n","    features = dict(zip(input_col_list_name, fields))\n","    labels = tf.stack([features[x] for x in output], axis=0)\n","    for i in output:\n","        features.pop(i)\n","\n","    return features, labels\n","\n","from functools import partial\n","def csv_to_dataset(csv_path, input_col_num_list,input_col_cat_list, \n","                   output_col_list, batch_size):\n","    reader = pd.read_csv(csv_path, chunksize=1)\n","    DF = reader.get_chunk()\n","\n","    default_types = []\n","    col_list = []\n","    col_list_name = []\n","\n","    for i in range(DF.columns.shape[0]):\n","        if DF.columns[i] in input_col_num_list:\n","            default_types.append([0.0])\n","            col_list_name.append(DF.columns[i])\n","            col_list.append(i)\n","        elif DF.columns[i] in input_col_cat_list:\n","            default_types.append(['0'])\n","            col_list_name.append(DF.columns[i])\n","            col_list.append(i)\n","        elif DF.columns[i] in output_col_list:\n","            default_types.append([0.0])\n","            col_list_name.append(DF.columns[i])\n","            col_list.append(i)\n","\n","    dataset = tf.data.TextLineDataset(csv_path).skip(1)\n","    dataset = dataset.shuffle(20000)\n","    partial_parse_line = partial(_parse_line, default_types=default_types, \n","                                 input_col_list=col_list, \n","                                 output=output_col_list,\n","                                 input_col_list_name=col_list_name)\n","    dataset = dataset.map(partial_parse_line, num_parallel_calls=4)\n","    dataset = dataset.batch(batch_size)\n","    return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mea3me6IboIa","colab_type":"text"},"source":["이제 csv 파일의 경로와 기타 패러미터를 이용하여 파이프라인을 생성합니다.\n","이후 DNN 모델을 생성하고 학습시켜봅니다."]},{"cell_type":"code","metadata":{"id":"JLMK9NuUbnbl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":728},"outputId":"76e328b6-7dee-41d1-a5ec-aff0f4cd0436","executionInfo":{"status":"ok","timestamp":1590672308258,"user_tz":-540,"elapsed":41585,"user":{"displayName":"Seungyo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiB3eHN6zrVATs5mQXXRkr2IZ7PrViaHKSjpj6P=s64","userId":"15265472381911721335"}}},"source":["train_dataset = csv_to_dataset(train_path, input_cols_num,input_cols_cat, \n","                               output_cols, 1024)\n","\n","model = keras.Sequential()\n","model.add(keras.layers.DenseFeatures(feature_columns))\n","model.add(keras.layers.Dense(30, activation='relu'))\n","model.add(keras.layers.Dense(30, activation='relu'))\n","model.add(keras.layers.Dense(30, activation='relu'))\n","model.add(keras.layers.Dense(30, activation='relu'))\n","model.add(keras.layers.Dense(1, activation=None))\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mse', metrics=['mape'])\n","model.fit(train_dataset, epochs= 10)\n","model.summary()\n","\n","train_predict= model.predict(train_dataset)\n","train_predict =pd.DataFrame(scaler.inverse_transform(train_predict), index= train_data.index, columns=['prediction'])\n","percentage_error = (train_predict['prediction'] - train_data['backup_vel']).abs()/ train_data['backup_vel']*100"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","99/99 [==============================] - 4s 36ms/step - loss: 0.0474 - mape: 2669.3384\n","Epoch 2/10\n","99/99 [==============================] - 4s 35ms/step - loss: 0.0021 - mape: 743.7119\n","Epoch 3/10\n","99/99 [==============================] - 4s 36ms/step - loss: 0.0016 - mape: 275.3106\n","Epoch 4/10\n","99/99 [==============================] - 3s 35ms/step - loss: 0.0015 - mape: 315.5370\n","Epoch 5/10\n","99/99 [==============================] - 3s 35ms/step - loss: 0.0015 - mape: 411.1433\n","Epoch 6/10\n","99/99 [==============================] - 4s 35ms/step - loss: 0.0015 - mape: 415.7964\n","Epoch 7/10\n","99/99 [==============================] - 3s 35ms/step - loss: 0.0014 - mape: 294.8199\n","Epoch 8/10\n","99/99 [==============================] - 4s 36ms/step - loss: 0.0014 - mape: 417.6372\n","Epoch 9/10\n","99/99 [==============================] - 4s 36ms/step - loss: 0.0014 - mape: 463.1200\n","Epoch 10/10\n","99/99 [==============================] - 3s 35ms/step - loss: 0.0014 - mape: 460.3730\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_features_1 (DenseFeatu multiple                  0         \n","_________________________________________________________________\n","dense_5 (Dense)              multiple                  330       \n","_________________________________________________________________\n","dense_6 (Dense)              multiple                  930       \n","_________________________________________________________________\n","dense_7 (Dense)              multiple                  930       \n","_________________________________________________________________\n","dense_8 (Dense)              multiple                  930       \n","_________________________________________________________________\n","dense_9 (Dense)              multiple                  31        \n","=================================================================\n","Total params: 3,151\n","Trainable params: 3,151\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"imqXAkJLcTDX","colab_type":"text"},"source":["test_data 에도 파이프라인을 생성하여 evaluate을 수행합니다."]},{"cell_type":"code","metadata":{"id":"QbI6Z-6icSxS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"175e8477-bbbf-4371-9148-bb8eb5736fe6","executionInfo":{"status":"ok","timestamp":1590672314308,"user_tz":-540,"elapsed":801,"user":{"displayName":"Seungyo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiB3eHN6zrVATs5mQXXRkr2IZ7PrViaHKSjpj6P=s64","userId":"15265472381911721335"}}},"source":["test_dataset = csv_to_dataset(test_path, input_cols_num,input_cols_cat,  output_cols, 1024)\n","\n","model.evaluate(test_dataset)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["1/1 [==============================] - 0s 7ms/step - loss: 0.0012 - mape: 3.4599\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.001202865387313068, 3.4598822593688965]"]},"metadata":{"tags":[]},"execution_count":9}]}]}