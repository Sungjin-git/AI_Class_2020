{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"2일차_TensorFlow Data API 를 활용한 정형 데이터 다루기.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"yRGBr-f3ZgHt","colab_type":"text"},"source":["# TensorFlow Data API 를 활용한 Categorical data 다루기\n","\n","지금까지의 예제는 numerical 데이터만 다루었으나\n","\n","categorical 데이터를 처리하게되는 경우가 많습니다.\n","\n","2개의 종류만 있는 데이터의 경우에는 0, 1로 표현하면 됩니다.\n","\n","많은 종류의 데이터를 쉽게 처리하기 위해\n","\n","TensorFlow 는 feature_column API 를 지원합니다.\n","\n","다만 Keras와 feature_column 을 동시에 사용하기 위해서는 \n","\n","또다른 API 인 Data API 를 필요로 합니다.\n","\n","Data API는 DNN 모델에 입력되는 데이터 파이프라인을 쉽게 다루도록 하는 API 입니다.\n","\n","메모리의 데이터와 파일로 저장된 데이터를 모두 지원하지만\n","\n","간단한 메모리의 데이터를 먼저 다루도록 하겠습니다."]},{"cell_type":"code","metadata":{"id":"9pAgLFlDZk8J","colab_type":"code","outputId":"5b784e93-7d98-4b7e-fab4-9cd5972e8eb3","executionInfo":{"status":"ok","timestamp":1590734649005,"user_tz":-540,"elapsed":1627,"user":{"displayName":"이성진","photoUrl":"","userId":"13873734164508447423"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WRpiwH4GZgHu","colab_type":"text"},"source":["categorical 한 데이터를 다루기 위해 속도 예측기의 역할을 좀더 확장해보겠습니다.\n","\n","기존에는 1개의 구간의 데이터만을 학습하였는데,\n","\n","이번에는 2개의 구간의 데이터를 학습하고, \n","\n","입력으로 주어지는 구간 ID ('V_ID') 에 따라\n","\n","예측을 하는 모델을 설계해보겠습니다.\n","\n","이전과는 다르게, input_cols 뿐 아니라 \n","\n","numerical column 은 input_cols_num,\n","\n","categorical column 은 input_cols_cat 으로 따로 지정한뒤 합하겠습니다.\n"]},{"cell_type":"code","metadata":{"id":"trsO6kK3ZgHv","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow import feature_column\n","#   set LABEL first\n","output_cols = ['vel']\n","#   set traffic column\n","input_cols_num = ['vel_t05', 'vel_t10', 'vel_t15', 'vel_t20',\n","               'vel_t25', 'vel_t30', 'vel_t35',  'vel_t40']\n","input_cols_cat = ['V_ID']\n","input_cols= input_cols_num + input_cols_cat\n","\n","path = './gdrive/My Drive/2020_AI_Class/03.Deep Learning/traffic_model/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6bTvT0YEZgHz","colab_type":"text"},"source":["2개의 구간의 속도 데이터 'traffic_data-2link.csv' 에\n","앞서 다루었던 training/test 데이터 나누기,\n","이상치 제거, scaling을 모두 적용하겠습니다.\n"]},{"cell_type":"code","metadata":{"id":"-3xnPlXCZgHz","colab_type":"code","colab":{}},"source":["traffic_data = pd.read_csv(path+'traffic_data_2link.csv', index_col = 0)\n","from sklearn.model_selection import train_test_split\n","\n","train_data, test_data = train_test_split(traffic_data, test_size = 1024)\n","vel_upper_limit = train_data['vel'].quantile(q=0.98)\n","train_data = train_data[train_data['vel']<=vel_upper_limit]\n","\n","from sklearn.preprocessing import MinMaxScaler\n","scaler= MinMaxScaler()\n","scaler.fit(train_data['vel'].values.reshape(-1,1))\n","\n","def normalize_numeric(dataframe, input_col_list, output_col_list):\n","    return_df = dataframe.copy()\n","    for col in input_col_list:\n","        return_df.loc[:, col] = pd.DataFrame(\n","            scaler.transform(return_df[col].values.reshape(-1, 1)),\n","            columns=[col], index=return_df.index)\n","    for col in output_col_list:\n","        col_backup = 'backup_'+col\n","        return_df[col_backup] = return_df[col].copy()\n","    for col in output_col_list:\n","        return_df.loc[:, col] = pd.DataFrame(\n","            scaler.transform(return_df[col].values.reshape(-1, 1)),\n","            columns=[col], index=return_df.index)\n","    return return_df\n","\n","train_data = normalize_numeric(train_data, input_cols_num, output_cols)\n","test_data = normalize_numeric(test_data, input_cols_num, output_cols)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWo5p6JeZgH2","colab_type":"text"},"source":["이제 데이터프레임의 데이터를 Data API 에서 지원하는 dataset 형태로 \n","변환하는 함수 df_to_dataset 을 만듭니다.\n","\n","함수에 batch_size에 원하는 값을 넣어주어\n","minibatch의 크기를 지정하게됩니다.\n"]},{"cell_type":"code","metadata":{"id":"DXWs4KO2ZgH2","colab_type":"code","colab":{}},"source":["def df_to_dataset(dataframe, input_col_list, output_col_list,\n","                  batch_size, training=True):\n","\n","    dataset = tf.data.Dataset.from_tensor_slices(\n","        (dict(dataframe[input_col_list]), \n","         dataframe[output_col_list].values ))\n","    if training :\n","      dataset = dataset.shuffle(buffer_size= len(dataframe))\n","    dataset = dataset.batch(batch_size= batch_size)\n","    return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"poSF7WOUZgH5","colab_type":"text"},"source":["함수가 제대로 동작하는지 확인해봅시다.\n"]},{"cell_type":"code","metadata":{"id":"zofuggM3ZgH5","colab_type":"code","outputId":"4fa0c521-a020-4aa9-fd2e-5681356f2bbe","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1590734696477,"user_tz":-540,"elapsed":1232,"user":{"displayName":"이성진","photoUrl":"","userId":"13873734164508447423"}}},"source":["train_dataset = df_to_dataset(train_data, input_cols, output_cols, 4)\n","\n","example_batch = next(iter(train_dataset))\n","\n","print(list(example_batch[0].keys()))\n","print(example_batch[0]['V_ID'])\n","print(example_batch[1] )"],"execution_count":12,"outputs":[{"output_type":"stream","text":["['vel_t05', 'vel_t10', 'vel_t15', 'vel_t20', 'vel_t25', 'vel_t30', 'vel_t35', 'vel_t40', 'V_ID']\n","tf.Tensor([b'0010VS00031' b'0010VS00029' b'0010VS00029' b'0010VS00029'], shape=(4,), dtype=string)\n","tf.Tensor(\n","[[0.87945586]\n"," [0.75821666]\n"," [0.18776686]\n"," [0.8589018 ]], shape=(4, 1), dtype=float64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2-fB0EveZgH9","colab_type":"text"},"source":["출력 결과의 첫번째 줄은 input의 column을 의미합니다.\n","두번째 줄은 categorical column 인 'V_ID' 의 값을 보여줍니다.\n","세번째 줄은 label을 보여줍니다."]},{"cell_type":"markdown","metadata":{"id":"E1VTw52FZgIB","colab_type":"text"},"source":["feature_column 은 모두  feature_columns 리스트에 저장하도록 합니다.\n","\n","숫자형 데이터는 numeric_column 으로 변환할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"PGfNttiBZgIB","colab_type":"code","colab":{}},"source":["feature_columns = []\n","for col in input_cols_num:\n","    feature_columns.append(feature_column.numeric_column(col))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_kIgY9c_ZgIE","colab_type":"text"},"source":["Categorical column 은 두가지 과정을 거쳐야 DNN이 인식할 수 있습니다.\n","\n","먼저 categorical_column_with_vocabulary_list 를 통해 category를 저장합니다.\n"]},{"cell_type":"code","metadata":{"id":"PGyiNLteZgIE","colab_type":"code","colab":{}},"source":["V_ID_list = train_data['V_ID'].unique().tolist()\n","V_ID_column = feature_column.\n","categorical_column_with_vocabulary_list('V_ID', V_ID_list)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rCYefkKPZgIH","colab_type":"text"},"source":["다음으로 indicator_column 을 통해 one hot encoding 을 하도록 한 뒤 feature_column에 저장됩니다."]},{"cell_type":"code","metadata":{"id":"TmzyjIoJZgIH","colab_type":"code","colab":{}},"source":["feature_columns.append(feature_column.indicator_column(V_ID_column))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3AUadZVfZgIJ","colab_type":"text"},"source":["feature_column 을 통해 변환된 값을 출력하는 함수 demo를 작성하고 그 결과를 확인해봅시다.\n"]},{"cell_type":"code","metadata":{"id":"X-D4mCmdZgIK","colab_type":"code","outputId":"59e8a75b-a27a-4e64-9880-55f9e7306f8f","colab":{}},"source":["def demo(feature_column_list):\n","  feature_layer = keras.layers.DenseFeatures(feature_column_list)\n","  print(feature_layer(example_batch[0]).numpy())\n","\n","demo(feature_columns)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer dense_features_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","[[0.         1.         0.83574975 0.8021847  0.8381331  0.78699106\n","  0.8900695  0.8393247  0.83535254 0.92075473]\n"," [1.         0.         0.8764647  0.85620654 0.8797418  0.8672294\n","  0.87884796 0.858292   0.83515394 0.8015889 ]\n"," [0.         1.         0.8512413  0.83396226 0.82502484 0.8613704\n","  0.86305857 0.8398213  0.866137   0.8308838 ]\n"," [0.         1.         0.8266137  0.83763653 0.81618667 0.8182721\n","  0.87070507 0.84399205 0.8397219  0.8180735 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nnd3vrwTZgIP","colab_type":"text"},"source":["Warining 은 무시하고 결과만 살펴보면 다음과 같습니다.\n","\n","각 행의 앞의 0 과 1로만 구성된 두 숫자는 V_ID가 V_ID_list의 순서대로 One hot encoding 이 된 결과입니다.\n","0010VS_00031 은 [1, 0]\n","0010VS_00029 은 [0, 1] 로 표기됩니다.\n","\n","뒤의 숫자들은 vel_t05 부터 vel_t40 까지의 값입니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pW1FuuuPZgIQ","colab_type":"text"},"source":["이제 batch_size를 큰 값으로 늘린뒤에 DNN을 컴파일해봅시다.\n"]},{"cell_type":"code","metadata":{"id":"V9Rn_ChpZgIQ","colab_type":"code","colab":{}},"source":["train_dataset = df_to_dataset(train_data, input_cols, output_cols,\n","                              1024)\n","\n","model = keras.Sequential()\n","model.add(keras.layers.DenseFeatures(feature_columns))\n","model.add(keras.layers.Dense(30, activation='relu'))\n","model.add(keras.layers.Dense(30, activation='relu'))\n","model.add(keras.layers.Dense(30, activation='relu'))\n","model.add(keras.layers.Dense(30, activation='relu'))\n","model.add(keras.layers.Dense(1, activation=None))\n","\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mse', metrics=['mape'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5f2kt_k_ZgIS","colab_type":"text"},"source":["이전에 사용한 Sequential API와 차이는 다음과 같습니다.\n","\n","1. DenseFeatures 를 통해 feature_column 을 입력으로 전달함\n","2. 첫 레이어인 DenseFeatures 에는 input_size 가 없음.\n","\n","현재 compile은 되어있으나, DenseFeatures를 사용하는 경우, \n","input size를 현재 모델이 알 수가 없어 fit 전에는 summary를 출력할 수 없습니다.(가능한 방법이 있음)\n","\n","이제 모델을 학습시켜 봅니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"MGLknU0VZgIT","colab_type":"code","outputId":"4cd7c5a7-9f90-45dd-cf34-a3fc676bafb6","colab":{}},"source":["model.fit(train_dataset, epochs= 10)\n","model.summary()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train for 99 steps\n","Epoch 1/10\n","99/99 [==============================] - 2s 22ms/step - loss: 0.0445 - mape: 1095.2314\n","Epoch 2/10\n","99/99 [==============================] - 1s 13ms/step - loss: 0.0017 - mape: 492.9348\n","Epoch 3/10\n","99/99 [==============================] - 1s 14ms/step - loss: 0.0016 - mape: 279.1785\n","Epoch 4/10\n","99/99 [==============================] - 1s 13ms/step - loss: 0.0015 - mape: 186.0617\n","Epoch 5/10\n","99/99 [==============================] - 1s 14ms/step - loss: 0.0015 - mape: 33.5221\n","Epoch 6/10\n","99/99 [==============================] - 1s 13ms/step - loss: 0.0015 - mape: 140.1088\n","Epoch 7/10\n","99/99 [==============================] - 1s 13ms/step - loss: 0.0015 - mape: 92.9295\n","Epoch 8/10\n","99/99 [==============================] - 1s 14ms/step - loss: 0.0014 - mape: 45.3461\n","Epoch 9/10\n","99/99 [==============================] - 1s 14ms/step - loss: 0.0014 - mape: 141.1759\n","Epoch 10/10\n","99/99 [==============================] - 1s 14ms/step - loss: 0.0014 - mape: 23.9314\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_features_8 (DenseFeatu multiple                  0         \n","_________________________________________________________________\n","dense (Dense)                multiple                  330       \n","_________________________________________________________________\n","dense_1 (Dense)              multiple                  930       \n","_________________________________________________________________\n","dense_2 (Dense)              multiple                  930       \n","_________________________________________________________________\n","dense_3 (Dense)              multiple                  930       \n","_________________________________________________________________\n","dense_4 (Dense)              multiple                  31        \n","=================================================================\n","Total params: 3,151\n","Trainable params: 3,151\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XQh45XFNZgIV","colab_type":"text"},"source":["마찬가지로 퍼센트 에러의 계산과 test data 에 대한 evaluation 결과를 확인하겠습니다.\n"]},{"cell_type":"code","metadata":{"id":"GhpXIiRPZgIW","colab_type":"code","outputId":"127d2792-5be6-40fc-8df6-28b3e483c9aa","colab":{}},"source":["train_predict= model.predict(train_dataset)\n","\n","train_predict =pd.DataFrame(scaler.inverse_transform(train_predict), index= train_data.index, columns=['prediction'])\n","percentage_error = (train_predict['prediction'] - train_data['backup_vel']).abs()/ train_data['backup_vel']*100\n","percentage_error"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["90247      0.628459\n","134585     0.861531\n","30660      0.691448\n","43662      2.643143\n","51682     11.990841\n","            ...    \n","83891      7.502252\n","55330      4.720992\n","78597      6.508076\n","93770      8.655577\n","169727    12.493389\n","Length: 100465, dtype: float64"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"05UhhYnLZgIY","colab_type":"code","outputId":"0b0d3dd2-37d0-49fd-a488-214ace12c5f4","colab":{}},"source":["percentage_error.mean()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15.788795751400574"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"0wq1AY2qZgIa","colab_type":"code","outputId":"edb8b1a1-e566-4ced-8d58-e6c39cadc860","colab":{}},"source":["test_dataset = df_to_dataset(test_data, input_cols, output_cols, 1024,training=False)\n","\n","model.evaluate(test_dataset)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r1/1 [==============================] - 0s 336ms/step - loss: 0.0014 - mape: 3.3777\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.0013645220315083861, 3.3777223]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"QkDDzFF-ZgId","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}